---
title: "Building intuition into popgen fundamentals and inference"
subtitle: "Exercises for the [Workshop on population and speciation genomics](http://evomics.org/workshops/workshop-on-population-and-speciation-genomics/2025-workshop-on-population-and-speciation-genomics-cesky-krumlov/)"
author:
  - "Martin Petr"
  - "[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)"
date: "January 2025"
date-format: "MMMM YYYY"
format:
  html:
    toc-title: Contents
    toc: true
    echo: true
    code-line-numbers: false
    fig-align: center
    self-contained: true
    callout-appearance: minimal
filters: 
 - collapse-callout.lua
---

<!-- TODO: Looks at these links to define a custom "Show the solution" callout -->

<!-- https://www.youtube.com/watch?v=DDQO_3R-q74 -->

<!-- https://examples.quarto.pub/collapse-callout/ -->


```{r}
#| echo: FALSE
RERUN <- FALSE
```




























# Reference materials

As you will be working on the exercises below (all of which focus on the 
popgen simulation R package [_slendr_](https://slendr.net)), some reference
materials will be  useful:

1. First and foremost, you should refer to the **slides with the _slendr_ "crash
course"** which we started our activity session with, and which cover pretty
much every bit of _slendr_ functionality you will need. You can find the material
rendered as [normal slides](https://bodkan.quarto.pub/cesky-krumlov-2025-slides/)
or [one-page handouts](https://bodkan.quarto.pub/cesky-krumlov-2025-handouts/)
(the latter will be a bit more practical for reference, as you won't have to
search for information by flipping through individual slides). If these Quarto Pub links
are not working, it could be because their servers are overloaded. In this case,
you can simply open `slides.html` or `handouts.html` from the
[GitHub repository](https://github.com/bodkan/cesky-krumlov-2025/) for this activity (you will have them also in the cloud -- instructions below).

2. **[Manual pages](https://www.slendr.net/reference/index.html) of all available
_slendr_ functions**. Note that you can get the help page of every _slendr_ R
`function()` by typing `?function` in the R console, and read it directly in
the RStudio. For instance, typing
`?ts_tajima` gives you the help page of the _tskit_/_slendr_ function implementing
the tree-based computation of Tajima's D. Whenever I discuss an optional parameter
of some function (or whenever you want to modify a function's behavior), this
is where you can find more information about it.

3. Relevant **tutorials on the [_slendr_ website](https://www.slendr.net)** which
you can find under the "Articles" link in the header of the webpage. In general,
the website contains much more detailed information which is available in the
[handouts](https://bodkan.quarto.pub/cesky-krumlov-2025-handouts/) in a much more
condensed form. I think this is probably _too much information_ for this
activity, but don't hesitate to read a bit more if you're interested!





























# Installation setup

**If you're using the RStudio browser-based cloud instance provided by
the workshop organizes, you don't actually
have to do anything! Just log in to the RStudio session in your browser and
navigate to the directory for this activity via `File -> Open Project...`
in the RStudio menu (selecting the `cesky-krumlov-2025.Rproj` project file
in that directory with open a fresh R session for that project in your
RStudio).**

If you want to open the most up-to date resources from the cloud instance
directly (rather than from the quarto.pub hosting service accessible via the
links to `handouts.html` and `exercises.html` above), **you might want to run
this _inside the directory with this activity's files on the cloud_**:

```
# first go to the directory with the activity files
cd <path to the activity directory on the cloud>

# then pull the latest changes from GitHub
git pull https://github.com/bodkan/cesky-krumlov-2025 main
```

::: callout-note
#### Setup on your own computer

**If you want to set up everything on your local machine from scratch
(maybe after the workshop is over and you want to return to the material on
yor own), just follow the couple of steps right below.**

1. **Clone the repository with the activity materials**. In a shell terminal on
Linux or macOS, in your home directory (or anywhere else, really) you can run:

    ```         
    $ git clone https://github.com/bodkan/cesky-krumlov-2025 ~/slendr_activity
    ```

(For Windows users: You can download a zip file with the whole course
even without git.)

2. **Open RStudio and install all of the following dependencies:**

    ```         
    > install.packages(c("slendr", "combinat", "cowplot", "dplyr", "readr",
                         "tidyr", "ggplot2", "rmarkdown", "yaml"))
    ```

3. **Set up the Python environment used by the _slendr_ R package** for simulation and tree-sequence analysis (still in the R console!):

    ```         
    > slendr::setup_env(agree = TRUE)
    ```

    If everything worked, you should get an optimistic message saying:

    ```         
    ======================================================================
    Python environment for slendr has been successfuly created, and the R
    interface to msprime, tskit, and pyslim modules has been activated.

    In future sessions, activate this environment by calling init_env().
    =======================================================================
    ```

4. **Open your RStudio (unless you did the above steps already in RStudio)**, and you're good to go!

---

**If the `setup_env()` installation procedure fails, try the following:**

1. Delete the failed installation attempt:

```         
slendr::clear_env(force = TRUE, all = TRUE)
```

2. Try installing it again, this time using `pip` as a Python installation method (the default is `conda` which unfortunately fails fairly often):

```         
slendr::setup_env(agree = TRUE, pip = TRUE)
```

In every previous installments of this workshop, this is all that was needed to resolve problems.*

---

**Installing SLiM**

It's unclear whether we will manage to go through the entirety of the
selection-based exercise. However, to be able to do this, having SLiM at
least version 4.2 (and it being available in your unix `$PATH!`) is required. If this isn't possible for your, don't worry. You'll be able to do most of
that exercise even without SLiM by using cached results of the SLiM
simulation that I provided.

:::





























# Organization of the exercises

For each exercise, you will get a brief explanation of the problem at hand and
some information about functions that could be useful to solve the exercise.
**The concrete, specific task will be always written like this in bold. As you
work on each part of each exercises, look for these bold lines.**

**Your goal for each exercise is to write a complete R script script (in
RStudio `File -> New file -> R script`). I suggest
you save the script for each exercise as `exercise1.R`, `exercise2.R`, etc.,
just to keep things tidy and easy to troubleshoot if needed.**

**Each exercise is composed of individual _parts_, which are designed to build
one upon the other in the order they are specified. As you progress through
sequential parts of each exercise, you will be adding code to your script for
that exercise.**

#### Don't worry about the number of exercises you're _"supposed to do"_

How far we manage to crunch through the content of this session will depend on
everybody's level of confidence in programming _and_ population genetics.
The contents of the activities as a whole are designed so that no matter
the moment at which we run out of time, you will take home enough knowledge of
_slendr_ to be able to immediately apply it to your own projects.

**To avoid anyone getting overwhelmed, I'll be stopping the activity at
some "checkpoints", to check in with how everything is going and quickly go
through the example solutions for the parts until that point, and give a quick
explanation.**

If you find yourself way ahead, there are bonus exercises (which I will not be
going through with everyone) -- those should provide at entertainment and
a bit more challenge for you while the rest of us are catching up. If you're
_really_ far ahead, just continue to the next exercise without waiting for
the rest of us.

#### Note on the programming aspect of the exercises

All the exercises will involve "real coding"! If you've never really programmed
entire scripts before, this could feel a little intimidating. Don't worry.
If you're ever lost, just take a peek at the solutions which are (by default
hidden) under each exercise part. Always try to work on a solution on your
own, but never let this be a barrier to your progress. Feel free to copy-paste
bits of my solutions into your own scripts.

If you find yourself [_totally lost_](https://scryfall.com/card/gtc/54/totally-lost),
don't hesitate to read my solutions from the get go, copy-pasting them into your
own script in the RStudio, executing them line by line, and trying to understand
what's going on. **It's the understanding that we're after here. Whether or not
you can implement everything yourself from scratch does not actually matter at all.** (Nobody in history has learned to program from scratch. Everyone
started by copy-pasting code examples written by someone else.)































# Exercise 1: Programming demographic models with _slendr_

### Part 1: Building a demographic model in _slendr_

**Use functions such as `population()`, `gene_flow()`, and `compile_model()`,
which we discussed in the ["crash course"](https://bodkan.quarto.pub/cesky-krumlov-2025-handouts/) at the
start of this session, to program the following toy model of human demographic
history in _slendr_.** (Apologies for my bad handwriting and poor artistic
ability.)

![](images/intro_model1.png){width="50%"}

::: {.aside}
**Note:** You could easily program the model so that different ancestral
populations are represented by separate `population()` commands (i.e.,
your model would start with a population called "human_chimp_ancestor" from
which a "CHIMP" and "hominin_ancestor" populations would split at 6 Mya, etc.) but
generally this is too annoying to do and requires too much code.

Feel free to write the model so that "CHIMP" is the first population, then
"AFR" population splits from it at 6 Mya, etc... Although it probably isn't
the most accurate way to describe the real evolutionary history, it simplifies
the coding significantly.

<br>
[Mya = million years ago;  kya = thousand years ago]
:::


**Hint:** Start script `exercise1.R` script in your RStudio session using the following "template". Then add a sequence of appropriate `population()` calls using
the syntax from the introductory slides (using the `parent = <pop>` argument
for programming splits of daughter populations -- which will be all except
the CHIMP lineage in our example), etc.

```{r}
#| eval: false
#| code-fold: false
library(slendr)
init_env()

<... population definitions ...>
<... gene-flow definition ...>

model <- compile_model(
  populations = list(...),
  gene_flow = <...>,
  generation_time = 30
)
```

::: aside
**Note:** With _slendr_ you can specify time in whatever format is more convenient or readable for your model. For instance here, because we're dealing with historical events which are commonly expressed in times given as"years ago", we can write them in a decreasing order – i.e. 7Mya → 6Mya → ..., as shown above – or, in terms of R code, 7e6 (or 7000000), 6e6 (6000000), etc.

In a later example, you will see that you can also encode the events in the time
direction going "forward" (i.e., the first event starting in generation 1, a
following event in generation 42, and so on).
:::

**Hint:** Remember that _slendr_ is designed with interactivity in mind!
When you write a chunk of code (such as a command to create a population
through a population split, or model compilation to create a `model` object),
execute that bit of code in the R console and inspect the summary information
printed by evaluating the respective R object you just created. You can either
copy-pasted stuff from your script to the R console, or used a convenient 
RStudio shortcut like Ctrl+Enter (Linux and Windows), or Cmd+Enter (Mac).

::: callout-note
#### Click to see the solution

```{r}
#| collapse: true
library(slendr)
init_env()

# Chimpanzee outgroup
chimp <- population("CHIMP", time = 7e6, N = 5000)

# Two populations of anatomically modern humans: Africans and Europeans
afr <- population("AFR", parent = chimp, time = 6e6, N = 15000)
eur <- population("EUR", parent = afr, time = 70e3, N = 3000)

# Neanderthal population splitting at 600 ky ago from modern humans
# (becomes extinct by 40 ky ago)
nea <- population("NEA", parent = afr, time = 600e3, N = 1000, remove = 40e3)

# Neanderthal introgression event (3% admixture between 55-50 kya)
gf <- gene_flow(from = nea, to = eur, rate = 0.03, start = 55000, end = 50000)

# Compile the entire model into a single slendr R object
model <- compile_model(
  populations = list(chimp, nea, afr, eur),
  gene_flow = gf,
  generation_time = 30,
  path = here::here("data/introgression"),  # <--- don't worry about these two
  overwrite = TRUE, force = TRUE            # <--- lines of code (ask me if interested)
)
```
:::

### Part 2: Inspecting the model visually

To visualize a _slendr_ model, you use the function `plot_model()`.
**Plot your compiled `model` to make sure you programmed it correctly!**
Your figure should roughly correspond to my doodle above.

::: aside
**Note:** Plotting of models in _slendr_ can be sometimes a little wonky,
especially if many things are happening at once. When plotting your
model, experiment with arguments `log = TRUE`, `proportions = TRUE`,
`gene_flow = TRUE`. Check `?plot_model` for more information on these.
:::

::: callout-note
#### Click to see the solution

```{r}
#| collapse: false
plot_model(model)
plot_model(model, sizes = FALSE)
plot_model(model, sizes = FALSE, log = TRUE)
plot_model(model, sizes = FALSE, log = TRUE, proportions = TRUE)
```
:::

### Part 3: Simulating genomic data

Once you have a compiled _slendr_ model stored in an R variable (from now on,
`model` will always mean a variable containing a compiled _slendr_ model object relevant for the given exercise,
for simplicity), we can simulate data from it. By default, _slendr_ models
always produce a [tree sequence](https://tskit.dev/tutorials/what_is.html).

::: {.aside}
**Note:** Tree sequence provides an extremely efficient means to store and
work with genomic data at a massive scale. However, you can always get
simulated data even in [traditional file formats](https://www.slendr.net/reference/index.html#tree-sequence-format-conversion),
such as VCF, EIGENSTRAT, or a plain old table of ancestral/derived genotypes.

In this activity we will be only working with tree sequences, because it's much
easier and faster to get interesting statistics from it directly in R.
:::

There are two simulation engines built into _slendr_ implemented by functions
`msprime()` and `slim()`. For traditional, non-spatial, neutral demographic
models, the engine provided by the `msprime()` function is much more efficient,
so we'll be using that for the time being. However, from a popgen theoretical perspective, both simulation functions
will give you the same results for any given compiled _slendr_ model (up to
some level of stochastic noise, of course).

::: aside
**Note:** Yes, this means you don't have to write any _msprime_ (or SLiM) code
to simulate data from a _slendr_ model!
:::

Here's how you can use the function to simulate a tree sequence from the
model you've just created using `compile_model()` in your script:

```{r}
#| eval: false
#| code-fold: false
ts <- msprime(
  model,
  sequence_length = <length of sequence to simulate [as bp]>,
  recombination_rate = <uniform recombination rate [per bp per generation]>
)
```

You will be seeing this kind of pattern over and over again in this exercise, so
it's a good idea to keep it in mind.


**Hint:** The `msprime()` function has also arguments `debug` and `run` which can be extremely useful for debugging.

**Simulate a tree sequence from your compiled `model` using the `msprime()`
engine, storing it to a variable `ts` as shown right above.
Use `sequence_length = 1e6` (so 1 Mb of sequence) and `recombination_rate = 1e-8` (crossover events per base pair per generation). Then experiment with
setting `debug = TRUE` (this prints out _msprime_'s own debugging summary
which you might already be familiar with from your previous activity?) and
then `run = FALSE` (this prints out a raw command-line which can run a _slendr_
simulation in the shell).**

::: callout-note
#### Click to see the solution

```{r}
#| collapse: true
# This simulates a tskit tree sequence from a slendr model. Note that you didn't have
# to write any msprime or tskit Python code!
ts <- msprime(model, sequence_length = 1e6, recombination_rate = 1e-8)

# Setting `debug = TRUE` instructs slendr's built-in msprime script to print
# out msprime's own debugger information. This can be very useful for debugging,
# in addition to the visualization of the model as shown above.
ts <- msprime(model, sequence_length = 1e6, recombination_rate = 1e-8, debug = TRUE)

# For debugging of technical issues (with msprime, with slendr, or both), it is
# very useful to have the `msprime` function dump the "raw" command-line to
# run the simulation on the terminal using plain Python interpreter
msprime(model, sequence_length = 1e6, recombination_rate = 1e-8, run = FALSE)
```
:::

### Part 4: Inspecting the tree-sequence object

As we will see later, _slendr_ provides an R-friendly interface to
accessing a [subset of _tskit_'s functionality](https://tskit.dev/tskit/docs/stable/python-api.html)
for working with tree sequence and for computing various popgen statistics.

For now, **type out the `ts` object in the terminal – what do you see?** You
should get a summary of a tree-sequence object that you're familiar with from
your _msprime_ and _tskit_ activity earlier in the day.

::: {.aside}
This is a very important feature of _slendr_ -- when a simulation is
concluded (doesn't matter if it was a `slim()` or `msprime()` simulation),
you will get a normal _tskit_ object. In fact, the fact that _slendr_ supports
(so far, and likely always) only a "subset" of all of _tskit_'s functionality
isn't stopping you to write custom Python/_tskit_ processing code of a tree
sequence generated from a _slendr_ model. Under the hood, a _slendr_ simulation
_really is_ just an _msprime_ (or SLiM) simulation! It's just executed through
a simplified interface (and dare I say a bit more convenient, for some purposes).
:::

::: callout-note
#### Click to see the solution

```{r}
# Typing out the object with the result shows that it's a good old tskit
# tree-sequence object
ts
```
:::

The brilliance of the tree-sequence data structure rests on its elegant
table-based implementation (much more information on that is [here](https://tskit.dev/tskit/docs/stable/data-model.html)). _slendr_ isn't
really designed to run very complex low-level manipulations of tree-sequence
data (it's strength lies in the convenient interface to popgen statistical
functions implemented by _tskit_), but it does contain a couple of functions
which can be useful for inspecting the lower-level nature of a tree sequence.
Let's look at a couple of them now.

**Use the `ts_table` function to inspect the low-level table-based representation of a tree sequence.** For instance, you can get the table of nodes with `ts_table(ts, "nodes")`, edges with `ts_table(ts, "edges")`, and do the same thing for "individuals", "mutations", and "sites". **Does your tree sequence contain any mutations? If not, why, and how can we even do any popgen with data without any mutations? As you're doing this, take a look at at the following
figure (this was made from a different tree sequence than you have, but that's
OK) to help you relate the information in the tables to a tree sequence which
those tables (particularly tables of nodes and edges) implicitly encode.**

::: callout-note
#### Click to see the solution

```{r}
# slendr provides a helper function which allows access to all the low-level
# components of every tree-sequence object
ts_table(ts, "nodes")
ts_table(ts, "edges")
ts_table(ts, "individuals")
# We didn't simulate any mutations, so we only have genealogies for now.
ts_table(ts, "mutations")
ts_table(ts, "sites")
```
:::

There are also two _slendr_-specific functions called `ts_samples()` (which
retrieves the "symbolic names" and dates of all recorded individuals at
the end of a simulation) and `ts_nodes()`. **You can run them simply as
`ts_samples(ts)` and `ts_nodes(ts)`. How many individuals (samples) are
in your tree sequence as you simulated it? How is the result of `ts_nodes()`
different from `ts_samples()`?**

::: callout-note
#### Click to see the solution

```{r}
# slendr provides a convenient function `ts_samples()` which allows us to
# inspect the contents of a simulated tree sequence in a more human-readable,
# simplified way. We can see that our tree sequence contains a massive number
# of individuals. Too many, in fact -- we recorded every single individual alive
# at the end of our simulation, which is something we're unlikely to be ever lucky
# enough to have, regardless of which species we study.
ts_samples(ts)
ts_samples(ts) %>% nrow()

library(dplyr)
ts_samples(ts) %>% group_by(pop) %>% tally

# This function returns a table similar to the one produced by `ts_table(ts, "nodes")`
# above, except that it contains additional slendr metadata (names of individuals
# belonging to each node, spatial coordinates of nodes for spatial models, etc.).
# It's a bit more useful for analyzing tree-sequence data than the "low-level" functions.
ts_nodes(ts)
```
:::

### Part 5: Scheduling sampling events

In the table produced by the `ts_samples()` function you saw that the tree
sequence we simulated recorded _everyone_. It's very unlikely, unless we're
extremely lucky, that we'll ever have a sequence of every single individual
in a population that we study. To get a little closer to the scale of the
genomic data that we usually work with on a day-to-day basis, we can restrict
our simulation to only record a subset of individuals.

We can precisely define which individuals (from which populations, and at which times) should be recorded in a tree sequence using the _slendr_ function `schedule_sampling()`. For instance, if we have a `model` with some _slendr_ populations in variables `eur` and `afr`, we can schedule the recording of 5 individuals from each at times 10000 (years ago) and 0 (present-day) (using
the "years before present" direction of time in our current model of
Neanderthal introgression) with the following code:

```{r}
#| eval: false
pop_schedule <- schedule_sampling(model, times = c(10000, 0), list(eur, 5), list(afr, 5))
```

This function simply returns a data frame. As such, we can create multiple of such schedules (of arbitrary complexity and granularity), and then bind them together into a single sampling schedule with a single line of code, like this:

```{r}
#| eval: false

# Note that the `times =` argument of the `schedule_sampling()` function can be
# a vector of times like here...
ancient_times <- c(40000, 30000, 20000, 10000)
eur_samples <- schedule_sampling(model, times = ancient_times, list(eur, 1))

# ... but also just a single number like here
afr_samples <- schedule_sampling(model, times = 0, list(afr, 1), list(eur, 42))
nea_samples <- schedule_sampling(model, time = 60000, list(nea, 1))

# But whatever the means you create the individual sampling schedules with,
# you can always bind them all to a single table with the `rbind()` function
schedule <- rbind(eur_samples, afr_samples, nea_samples)
schedule
```

**Using the function `schedule_sampling` (and with the help of `rbind` as
shown in the previous code chunk), program the sampling of the following
sample sets at given times, saving it to variable called `schedule`:**

| time  | population(s)   | \# individuals |
|-------|:----------------|----------------|
| 70000 | nea             | 1              |
| 40000 | nea             | 1              |
| 0     | chimp, afr, eur | 5              |

**Additionally, schedule the sampling of a single `eur` individual at the
following times:**

```{r}
t <- seq(40000, 2000, by = -2000)
```

::: aside
**Note:** You can provide a vector variable (such as `t` in this example) as the
`times =` argument of `schedule_sampling()`.
:::

**In total, you should schedule the recording of 38 individuals.**

::: callout-note
#### Click to see the solution

```{r}
# Here we scheduled the sampling of two Neanderthals at 70kya and 40kya
nea_samples <- schedule_sampling(model, times = c(70000, 40000), list(nea, 1))
nea_samples # (this function produces a plain old data frame!)

# Here we schedule one Chimpanzee sample, 5 African samples, and 10 European samples
present_samples <- schedule_sampling(model, times = 0, list(chimp, 1), list(afr, 5), list(eur, 10))

# We also schedule the recording of one European sample between 50kya and 2kya,
# every 2000 years
times <- seq(40000, 2000, by = -2000)
emh_samples <- schedule_sampling(model, times, list(eur, 1))

# Because those functions produce nothing but a data frame, we can bind
# individual sampling schedules together
schedule <- rbind(nea_samples, present_samples, emh_samples)
schedule
```
:::

**Then, verify the correctness of your overall sampling `schedule` by visualizing
it together with your `model` like this:**

::: aside
**Note:** As you've seen above, the visualization is often a bit wonky and convoluted with overlapping elements and it can be even worse with samples added, but try to experiment with arguments to `plot_model` described above to make the plot a bit more helpful for sanity checking.
:::

```{r}
#| eval: false
plot_model(model, samples = schedule)
```

::: callout-note
#### Click to see the solution

```{r}
plot_model(model, sizes = FALSE, samples = schedule)
```

```{r}
plot_model(model, sizes = FALSE, log = TRUE, samples = schedule)
```
:::

### Part 6: Simulating a defined set of individuals

You have now both a compiled _slendr_ `model` and a well-defined sampling `schedule`.

**Use your combined sampling schedule stored in the `schedule` variable to run a
new tree-sequence simulation from your model (again using the `msprime()` function),
this time restricted to just those individuals scheduled for recording. You can
do this by providing the combined sampling `schedule` as the `samples = schedule` argument
of the function `msprime` you used above.** Just replace the line(s) with your first
`msprime()` from the previous part of this exercise with the new one, which
uses the `schedule` for your customized sampling.

**Also, while you're doing this, use the `ts_mutate()` function to add
overlay neutral mutations on the simulated tree sequence right after the
`msprime()` call.** (Take a look at the handounts for a reminder of the `%>%`
pipe patterns I showed you.)

::: callout-note
#### Click to see the solution

```{r}
#| echo: false
model <- read_model(here::here("data/introgression"))
ts <- ts_read(file = here::here("data/introgression.trees"), model = model)
```

```{r}
#| eval: false
# The command below will likely take a few minutes to run, so feel free to go
# down from 100 Mb sequence_length to even 10Mb (it doesn't matter much).
# (The `random_seed =` argument is there for reproducibility purposes.)
ts <-
  msprime(model, sequence_length = 100e6, recombination_rate = 1e-8, samples = schedule, random_seed = 1269258439) %>%
  ts_mutate(mutation_rate = 1e-8, random_seed = 1269258439)
# Time difference of 2.141642 mins

# If you're bothered by ho long this takes, feel free to call these two lines
# to 100% reproduce my results without any expensive computation:
model <- read_model(here::here("data/introgression"))
ts <- ts_read(here::here(file = "data/introgression.trees"), model = model)

# We can save a tree sequence object using a slendr function `ts_write` (this
# can be useful if we want to save the results of a simulation for later use).
dir.create("data", showWarnings = FALSE)
ts_write(ts, "data/introgression.trees")
```
:::

**Inspect the tree-sequence object saved in the `ts` variable by typing
it into the R console again (this interactivity really helps with catching
nasty bugs early during the programming of your script). You can
also do a similar thing via the table produced by the `ts_samples()` function.
You should see a much smaller number of individuals being recorded, indicating
that the simulation was much more efficient and produced genomic data for
only the individuals of interest.**

::: aside
**Note:** When you think about it, it's actually quite astonishing how fast
_msprime_ and _tskit_ are when dealing with such a huge amount of sequence
data from tens of thousands of individuals on a simple laptop!
:::

::: callout-note
#### Click to see the solution

```{r}
# Inspect the (tskit/Python-based) summary of the new tree sequence
ts

# Get the table of all recorded samples in the tree sequence
ts_samples(ts)

# Compute the count of individuals in different time points
library(dplyr)

ts_samples(ts) %>% group_by(pop, time == 0) %>% tally %>% select(pop, n)
```
:::


<!-- End of Bonus exercises for Exercise 1 -->


























# Exercise 2: Computing popgen statistics on tree sequences from _slendr_

In this exercise, you will build on top of the results from Exercise 1.
Specifically, we will learn how to compute popgen statistics on _slendr_-simulated
tree sequences using
[_slendr_'s interface](https://www.slendr.net/reference/index.html#tree-sequence-statistics)
to the _tskit_ Python module.

First, create a new R script `exercise2.R` and paste in the following code. This
is one of the possible solutions to the Exercise 1, and it's easier if we all
use it to be on the same page from now on, starting from the same model and
the same simulated tree sequence:

```{r}
#| collapse: true
library(slendr)
init_env()

chimp <- population("CHIMP", time = 7e6, N = 5000)
afr <- population("AFR", parent = chimp, time = 6e6, N = 15000)
eur <- population("EUR", parent = afr, time = 70e3, N = 3000)
nea <- population("NEA", parent = afr, time = 600e3, N = 1000, remove = 40e3)

gf <- gene_flow(from = nea, to = eur, rate = 0.03, start = 55000, end = 50000)

model <- compile_model(
  populations = list(chimp, nea, afr, eur),
  gene_flow = gf,
  generation_time = 30
)

# We will read a cached version of a tree sequence to make sure we're all on
# the same page. That said, if you managed to do Exercise 1 on your own, feel
# free to stick with your own simulated tree sequence!
ts <- ts_read(here::here("data/introgression.trees"), model = model)

cowplot::plot_grid(
  plot_model(model, proportions = TRUE),
  plot_model(model, proportions = TRUE, log = TRUE),
  nrow = 1
)
```

As a sanity check, let's use a couple of tidyverse table-munging tricks
to make sure the tree sequence does contain a set of sample
which matches our intended sampling schedule (particularly the time series
of European individuals and the two Neanderthals):

```{r}
library(dplyr)

# total number of recorded individuals in the tree sequence
ts_samples(ts) %>% nrow
# times of sampling of each recorded European individual
ts_samples(ts) %>% filter(pop == "EUR") %>% pull(time)
# times of sampling of each recorded Neanderthal
ts_samples(ts) %>% filter(pop == "NEA") %>% pull(time)
# count of individuals in each population
ts_samples(ts) %>%
  group_by(pop, present_day = time == 0) %>%
  tally %>%
  select(pop, present_day, n) %>%
  arrange(present_day)
```

::: {.aside}
**Note:** These bits of tidyverse code are extremely helpful when you're working
with large tree sequences with many individuals as sanity checks that your
sampling worked as intended. I'm listing them here in case you've never worked
with the tidyverse family of R packages before (such as the _dplyr_ package
where `filter()`, `group_by()`, `tally()`, and `pull()` come from).
:::

Everything looks good! Having made sure that the `ts` object contains the
individuals we want, let's move to the exercise.

## Part 1: Computing nucleotide diversity

The toy model of ancient human history plotted above makes a fairly clear prediction of what would be the
nucleotide diversity expected in the simulated populations.
**Compute the nucleotide diversity in all populations using the _slendr_ function
[`ts_diversity()`](https://www.slendr.net/reference/ts_diversity.html#ref-examples) in your tree sequence `ts`. Do you get numbers that (relatively between
all populations) match what would expect from the model given the $N_e$ that
you programmed for each?**

**Hint:** Nearly every _slendr_ statistic function interfacing with _tskit_
accepts a `ts` tree-sequence object as its first argument, with further arguments
being either a vector of individual names representing a group of samples to
compute a statistic on, or a (named) list of such vectors (each element of that
list for a group of samples) -- these lists are intended to be equivalent to
the `sample_sets =` argument of many _tskit_ Python methods (which you've learned
about in your activity on _tskit_), except that they allow symbolic names
of individuals, rather then integer indices of nodes in a tree sequence.

Although you can get all the above information by processing the table produced
by the `ts_samples()` function, _slendr_ provides a useful helper function
`ts_names()` which only returns the names of individuals as a vector 
(or a named list of such vectors, one vector per population as shown below).

When you call it directly, you get a plain vector of individual names:

```{r}
ts_names(ts)
```

This is not super helpful, unless we want to compute some statistic for _everyone_
in the tree sequence, regardless of their population assignment. Perhaps a bit
more useful is to call the function like this, because it will produce a result
which can be immediately used as the `sample_sets =` argument mentioned in the
**Hint** above:

```{r}
ts_names(ts, split = "pop")
```

As you can see, this gave us a normal R list, with each element containing
a vector of individual names in a population. Note that we can use standard R
list indexing to get subsets of individuals:

```{r}
names <- ts_names(ts, split = "pop")

names["NEA"]

names[c("EUR", "NEA")]
```

etc.

Many of the following exercises will use these kinds of tricks to instruct
various _slendr_ / _tskit_ functions to compute statistics on subsets of
all individuals sub-sampled in this way.

**After you computed the nucleotide diversity per-population, compute it for
each individual separately using the same function `ts_diversity()`** (which,
in this setting, gives you effectively the heterozygosity for each individual).

**Hint:** You can do this by giving a vector of names as `sample_sets =` (so
not an R list of vectors). Feel free to take a look at the solution if you
find this distinction confusing.

::: callout-note
### Click to see the solution

**Population-based nucleotide diversity:**

```{r}
# Let's first get a named list of individuals in each group we want to be
# working with (slendr tree-sequence statistic functions generally operate
# with this kind of structure)
sample_sets <- ts_names(ts, split = "pop")
sample_sets

# We can use such `sample_sets` object to compute nucleotide diversity (pi)\
# in each population, in a bit of a similar manner to how we would do it
# with the standard tskit in Python
pi_pop <- ts_diversity(ts, sample_sets = sample_sets)
arrange(pi_pop, diversity)
```

You can see that this simple computation fits  the extreme differences in
long-term $N_e$ encoded by your _slendr_ demografr model.

**Per-individual heterozygosity:**

We can do this by passing the vector of individual names directory as the `sample_sets =` argument, rather than in a list of groups as we did above.

```{r}
# For convenience, we first get a table of all individuals (which of course
# contains also their names) and in the next step, we'll just add their
# heterozygosities as a new column.
pi_df <- ts_samples(ts)
pi_df$name

pi_df$diversity <- ts_diversity(ts, sample_sets = pi_df$name)$diversity
pi_df

# Let's plot the results using the ggplot2 package
# (because a picture is worth a thousand numbers!)
library(ggplot2)

ggplot(pi_df, aes(pop, diversity, color = pop, group = pop)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter() +
  theme_bw()
```
:::

## Part 2: Computing pairwise divergence

**Use the function
[`ts_divergence()`](https://www.slendr.net/reference/ts_divergence.html#ref-examples)
to compute genetic divergence between all pairs of populations. Again, do you
get results compatible with our demographic model in terms of expectation
given the split times between populations as you programmed them for your
model?**

**Hint:** Again, you can use the same concept of `sample_sets =` we discussed
in the previous part. In this case, the function computes _pairwise_
divergence between each element of the list given as `sample_sets =` (i.e.,
for each vector of individual names).

::: callout-note
### Click to see the solution

```{r}
sample_sets <- ts_names(ts, split = "pop")

div_df <- ts_divergence(ts, sample_sets)
arrange(div_df, divergence)
```
:::

## Part 3: Detecting Neanderthal admixture in Europeans

Let's now pretend its about 2008, we've sequenced the first Neanderthal genome,
and we are working on a project that will
[change human evolution research forever](https://www.science.org/doi/10.1126/science.1188021).
We also have the genomes of a couple of people from Africa and Europe, which we
want to use to answer the most burning question of all evolutionary
anthropology: _"Do some people living today carry Neanderthal ancestry?"_

Earlier you've learned about $f$-statistics of various kinds. You have also
heard that an $f_4$ statistic (or its equivalent $D$ statistic) can be used
as a test of "treeness". Simply speaking, for some "quartet" of individuals
or population samples, they can be used as a hypothesis test of whether the
history of those samples is compatible with there not having been an introgression.

**Compute the $f_4$ test of Neanderthal introgression in EUR individuals using
the _slendr_ function `ts_f4()`.** When you're running it, you will have to
provide individuals to compute the statistic on using a slightly different
format. Take a look at the help page available as `?ts_f4` for more information.
**When you're computing the $f_4$, make sure to set `mode = "branch"` argument
of the `ts_f4()` function (we will get to why a bit later).**

::: {.aside}
**Note:** By default, each _slendr_ / _tskit_ statistic function operates
on mutations, and this will switch them to use branch length (as you might
know, $f$-statistics are mathematically defined using branch lengths in trees
and `mode = "branch"` does exactly that).
:::

**Hint:** If you haven't learned this in your $f$-statistics lecture, you want
to compute (and compare) the values of these two statistics using the _slendr_
function `ts_f4()`:

1. $f_4$(\<some African\>, \<another African\>; \<Neanderthal\>, \<Chimp\>)

2. $f_4$(\<some African\>, \<a test European\>; \<Neanderthal\>, \<Chimp\>),

here <individual> can be the name of any individual recorded in your
tree sequence, such as names you saw as `name` column in the table returned
by `ts_samples(ts)` or the vectors of strings returned by `ts_names(ts)`.

To simplify things a lot, we can understand the above equations as comparing the
counts of so-called BABA and ABBA allele patterns between the quarted of samples
specified in the statistics:

$$
f_4(AFR, X; NEA, CHIMP) = \frac{\#BABA - \#ABBA}{\#SNPs}
$$

The first is not expected to give values "too different" from 0 because we
don't expect two African individuals to differ "significantly" in terms of how
much alleles they share with a Neanderthal. The other should -- if there was
a Neanderthal introgression into Europeans some time in their history -- be
"significantly negative".

**Is the second of those two statistics "much more negative" than the first,
as expected assuming introgression from Neanderthals into Europeans?**

**In the text right above, why am I putting "significantly" and "much more
negative" in quotes? What are we missing here for this being a true hypothesis
test as you might be accustomed to from computing** $f$-statistics using
a tool such as ADMIXTOOLS? (We will get to this again in the following part
of this exercise.)

::: callout-note
### Click to see the solution

```{r}
# Compute the difference in the amount of allele sharing between two African
# individuals and a Neanderthal
f4_null <- ts_f4(ts, W = "AFR_1", X = "AFR_2", Y = "NEA_1", Z = "CHIMP_1", mode = "branch")
f4_null

# Compute the difference in the amount of allele sharing between an African
# individual vs European individual and a Neanderthal
f4_alt <- ts_f4(ts, W = "AFR_1", X = "EUR_1", Y = "NEA_1", Z = "CHIMP_1", mode = "branch")
f4_alt

# We can see that the second test resulted in an f4 value about ~20 times more
# negative than the first test, indicating that a European in our test carries
# "significantly more" Neanderthal alleles compared to the baseline expectation
# of no introgression established by the comparison to an African ...
f4_alt$f4 / f4_null$f4

# ... although this is not a real test of significance (we have no Z-score or
# standard error which would give us something like a p-value for the hypothesis
# test, as we get by jackknife procedure in ADMIXTOOLS)
```
:::

## Part 5: Detecting Neanderthal admixture in Europeans v2.0

The fact that we don't get something equivalent to a p-value in these kinds of
simulations is generally not a problem, because we're often interested in
establishing a trend of a statistic under various conditions, and understanding
when and how it behaves in a certain way. If statistical noise is a problem, we
can easily work around it by computing a statistic on multiple simulation
replicates or even increasing the sample sizes. I used this approach quite
successfully on a related problem in [this paper](https://www.pnas.org/doi/10.1073/pnas.1814338116#fig02).

On top of that, p-value of something like an $f$-statistic (whether it's
significantly different from zero) is also strongly affected by quality of
the data, sequencing errors, coverage, etc. (which can certainly be done
using simulations!). However, these are an orthogonal issue which has little
to do with the underlying evolutionary model in question anyway -- and it's
the investigation of expected patterns of behavior of popgen statistics
what we're after in our exercises.

In short, how much is "significantly different from zero compared to some
baseline expectation" can be easily drowned in noise in a simple
single-individual comparisons as we did above. Let's increase the sample
size a bit to see if a statistical pattern becomes more apparent.

**Compute the first $f_4$ statistic of the baseline expectation between
a pair of Africans and the second $f_4$ statistic comparing an African and
a European, but this time on _all recorded Africans_ and _all recorded
Europeans_, respectively. Plot the *distributions* of those two sets of
statistics. This should remove lots of the uncertainty and make a statistical
trend stand out much more clearly.**

**Hint:** Whenever you need to compute something for many things in sequence, looping is very useful. One way to do compute, say, an $f_4$ statistic over many individuals is use this kind of pattern using R's looping function `lapply()`:

```{r}
#| eval: false

# Loop over vector of individual names (variable x) and apply a given ts_f4()
# expression on each individual (note the ts_f4(..., X = x, ...) in the code)
list_f4 <- lapply(
  c("ind_1", "ind_2", ...),
  function(x) ts_f4(ts, W = "AFR_1", X = x, Y = "NEA_1", Z = "CHIMP_1", mode = "branch")
)

# The above gives us a list of data frames, so we need to bind them all into a
# single table for easier interpretation and visualization
df_f4 <- do.call(rbind, list_f4)
```

::: callout-note
### Click to see the solution

```{r}
# Let's compute the f4 statistic for all Africans and Europeans to see the
# f4 introgression patterns more clearly
f4_afr <- lapply(sample_sets$AFR, function(x) ts_f4(ts, W = "AFR_1", X = x, Y = "NEA_1", Z = "CHIMP_1", mode = "branch")) %>% do.call(rbind, .)
f4_afr
f4_eur <- lapply(sample_sets$EUR, function(x) ts_f4(ts, W = "AFR_1", X = x, Y = "NEA_1", Z = "CHIMP_1", mode = "branch")) %>% do.call(rbind, .)
f4_eur

# Let's add population columns to each of the two results, and bind them together
# for plotting
f4_afr$pop <- "AFR"
f4_eur$pop <- "EUR"

# Bind both tables together
f4_results <- rbind(f4_afr, f4_eur)

# Visualize the results
f4_results %>%
  ggplot(aes(pop, f4, color = pop)) +
  geom_boxplot() +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = 2) +
  ggtitle("f4(AFR, EUR; NEA, CHIMP)") +
  theme_bw()
```
:::

::: callout-tip
## Bonus exercises

### Bonus 1: `mode = "branch"` vs `mode = "site"`

**Repeat the previous part of the exercise by setting `mode = "site"` in the 
`ts_f4()` function calls** (this is actually the default behavior of all
_slendr_ tree-sequence based _tskit_ functions). This will switch the _tskit_
computation to using mutation counts along each branch of the tree sequence,
rather than using branch length themselves. **Why might the branch-based computation
be a bit better if what we're after is investigating the expected values of statistics
under some model?**


::: callout-note
### Click to see the solution

See [this tutorial](https://tskit.dev/tutorials/no_mutations.html#genealogy-based-measures-are-less-noisy) (and particularly the directly linked section) for explanation.
:::


### Bonus 2: Outgroup $f_3$ statistic

**Use the function `ts_f3()` to compute the outgroup** $f_3$ statistic between pairs of African-European, African-Neanderthal, and European-Neanderthal and a Chimpanzee outgroup.

**Hint:** The $f_3$ statistic is traditionally expressed as $f_3(A, B; C)$, where C represents the outgroup. Unfortunately, in _tskit_ the outgroup is named A, with B and C being the pair of samples from which we trace the length of branches towards the outgroup, so the statistic is interpreted as $f_3(B, C; A)$.

**How do the outgroup f3 results compare to your expectation based on simple population relationships (and to the divergence computation above)?**

**Do you see any impact of introgression on the $f_3$ value when a Neanderthal is included in the computation?**

::: callout-note
### Click to see the solution

```{r}
# f3(A, B; C) = E[ (A - C) * (B - C) ]
# This means that in tskit, C is the outgroup (different from ADMIXTOOLS!)

# We can compute f3 for individuals...
ts_f3(ts, B = "AFR_1", C = "EUR_1", A = "CHIMP_1")

# ... but also whole populations (or population samples)
ts_f3(ts, B = sample_sets["AFR"], C = sample_sets["EUR"], A = "CHIMP_1")

ts_f3(ts, B = sample_sets["AFR"], C = sample_sets["NEA"], A = "CHIMP_1")

ts_f3(ts, B = sample_sets["EUR"], C = sample_sets["NEA"], A = "CHIMP_1")
```
:::

### Bonus 3: Outgroup $f_3$ statistic as a linear combination of $f_2$ statistics

You might have learned that any complex $f$-statistic can be expressed as a linear combination of multiple $f_2$ statistics (which represent simple branch length separating two lineages). **Verify that this is the case by looking up equation *(20b)* in [this amazing paper](https://academic.oup.com/genetics/article/202/4/1485/5930214) and compute an** $f_3$ statistic for any arbitrary trio of individuals of your choosing using this linear combination of $f_2$ statistics.

::: callout-note
### Click to see the solution

```{r}
# standard f3
ts_f3(ts, B = "AFR_1", C = "AFR_2", A = "CHIMP_1")

# a "homemade" f3 statistic as a linear combination of f2 statistics
# f3(A, B; C) = f2(A, C) + f2(B, C) - f2(A, B) / 2
homemade_f3 <- (
  ts_f2(ts, A = "AFR_1", B = "CHIMP_1")$f2 +
  ts_f2(ts, A = "AFR_2", B = "CHIMP_1")$f2 -
  ts_f2(ts, A = "AFR_1", B = "AFR_2")$f2
) / 2
homemade_f3
```
:::

### Bonus 4: Trajectory of Neanderthal ancestry in Europe over time

There used to be a lot of controversy about the question of whether or not did Neanderthal ancestry proportion in Europeans decline or not over the past 40 thousand years (see figure 1 in [this paper](https://www.pnas.org/doi/full/10.1073/pnas.1814338116) figure 2 in [this paper](https://www.nature.com/articles/nature17993)).

Your simulated tree sequence contains a time-series of European individuals over time. Use the _slendr_ function `ts_f4ratio()` to compute (and then plot) the proportion (commonly designated as `alpha`) of Neanderthal ancestry in Europe over time. Use $f_4$-ratio statistic of the following form:

```{r}
#| eval: false
ts_f4ratio(ts, X = <vector of ind. names>, A = "NEA_1", B = "NEA_2", C = "AFR_1", O = "CHIMP_1")
```

::: callout-note
### Click to see the solution

```{r}
# Extract table with names and times of sampled Europeans (ancient and present day)
eur_inds <- ts_samples(ts) %>% filter(pop == "EUR")
eur_inds

# Compute f4-ration statistic (this will take ~30s) -- note that we can provide
# a vector of names for the X sample set to the `ts_f4ratio()` function
nea_ancestry <- ts_f4ratio(ts, X = eur_inds$name, A = "NEA_1", B = "NEA_2", C = "AFR_1", O = "CHIMP_1")

# Add the age of each sample to the table of proportions
nea_ancestry$time <- eur_inds$time
nea_ancestry

nea_ancestry %>%
  ggplot(aes(time, alpha)) +
  geom_point() +
  geom_smooth(method = "lm", linetype = 2, color = "red", linewidth = 0.5) +
  xlim(40000, 0) +
  coord_cartesian(ylim = c(0, 0.1)) +
  labs(x = "time [years ago]", y = "Neanderthal ancestry proportion") +
  theme_bw() +
  ggtitle("Neanderthal ancestry proportion in Europeans over time")

# For good measure, let's test the significance of the decline using a linear model
summary(lm(alpha ~ time, data = nea_ancestry))
```
:::

### Bonus 5: How many unique f4 quartets are there?

In your lecture about $f$-statistics, you've probably learned about various symmetries in $f_4$ (but also other $f$-statistics) depending on the arrangement of the "quartet". As a trivial example, an $f_3(A; B, C)$ and $f_3(A; C, B)$ will give you exactly the same value, and the same thing applies even to more complex $f$-statistics like $f_4$.

**Use simulations to compute how manu unique** $f_4$ values involving a single quartet are there.

**Hint:** Draw some trees to figure out why could that be true. Also, when computing `ts_f4()`, set `mode = "branch"` to avoid the effect of statistical noise due to mutations.

::: callout-note
### Click to see the solution

```{r}
# # install a combinatorics R package
# install.packages("combinat")

library(combinat)

# These are the four samples we can create quartet combinations from
quartet <- c("AFR_1", "EUR_1", "NEA_1", "CHIMP_1")
quartets <- permn(quartet)
quartets

# How many permutations there are in total?
#   4! = 4 * 3 * 2 * 1 = 24
factorial(4)

# We should therefore have 24 different quartet combinations of samples
length(quartets)

# Loop across all quartets, computing the corresponding f4 statistic (we want
# to do this using branch lengths, not mutations, as the mutation-based computation
# would involve statistical noise)
all_f4s <- lapply(quartets, function(q) ts_f4(ts, q[1], q[2], q[3], q[4], mode = "branch"))

# Bind the list of f4 results into a single data frame and inspect the results
all_f4s <- bind_rows(all_f4s) %>% arrange(abs(f4))
print(all_f4s, n = Inf)

# Narrow down the results to only unique f4 values
distinct(all_f4s, f4, .keep_all = TRUE)
distinct(all_f4s, abs(f4), .keep_all = TRUE)
```
:::
::::::
<!-- End of Bonus exercises for Exercise 2 -->

































# Exercise 3: Simulation-based inference of $N_e$

What we did so far is learning how _slendr_ models provide an easy way to encode
demographic models in R and simulate (even very large!) tree sequences from them.
This allows us to very quickly verify our intuition about some popgen problem
(things like _"Hmmm, I wonder what would an $f_4$ statistic look like if my model
includes this particular gene-flow event?_), in just a few lines of R. We have
been able to even answer some questions like this directly in a meeting, pretty
much on the spot! This makes _slendr_ a very powerful "popgen calculator".

Now let's take things one step forward. Imagine you gathered some empirical
data, like an allele frequency spectrum (AFS) from a population that you
study. That data was, in the real world, produced by some (hidden) biological
process (demographic history) that we generally don't know anything about _a
priori_. For instance, the population we study had some $N_e$, which we don't
know the value of -- the only thing we have is the observed AFS.

Simulations can be a great tool to estimate the most likely value of such an
unknown parameter. Briefly speaking, and staying with this $N_e$ and AFS example,
we can simulate a large number of AFS vectors (each resulting from a different
assumed $N_e$ value) and then pick just those $N_e$ values (or just one $N_e$
value) which produced a simulated AFS closest to the observed AFS.
**This is exactly what you'll be doing just now in Exercise 3.**

## Part 1: A self-contained _slendr_ function of $N_e \rightarrow \textrm{AFS}$

**In a new script `exercise3.R` write a custom R function called `simulate_afs()`,
which will take `Ne` as its only parameter. Use this function to compute (and
return) [AFS vectors](https://en.wikipedia.org/wiki/Allele_frequency_spectrum)
for a couple of `Ne` values of your choosing, but staying between
`Ne = 1000` and `Ne = 30000` Plot those AFS vectors and observe how (and why?)
do they differ based on `Ne` parameter you used in each respective simulation.**

**Hint:** The function should create a one-population *forward-time* model
(our population starting at `time = 1`, with the model `simulation_length` 100000
and `generation_time` 1), simulate 10Mb tree sequence (recombination and mutation
rates of 1e-8), compute AFS for 10 samples and return it the AFS vector as its result.

**Hint:** If you've never programmed before, the concept of a "custom function" might
be very alien to you. If you need help, feel free to start building your
`exercise3.R` solution based on this "template" (just fill in missing relevant
bits of _slendr_ code that you should be already familiar with):

```{r}
#| eval: false
library(slendr)
init_env()

simulate_afs <- function(Ne) {
  # In here you should write code which will compile a single-population
  # demographic model based on the parameters specified above. It should
  # then simulate 10Mb of tree sequence using `msprime()`, compute an AFS
  # vector using the function `ts_afs()` for any subset of 10 individuals
  # in the tree sequence, and finally return that vector.

  return(result) # `result` is a variable with your 10-sample AFS, return it
}

afs_1 <- simulate_afs(Ne = 1000) # simulate AFS from a Ne = 1000 model...
plot(afs_1, type ="o")           # ... and plot it
```

**Note:** Remember that you should drop the first element of the AFS vector produced by `ts_afs()` (for instance with something like `result[-1]` if `result` contains the output of `ts_afs()`) technical reasons related to _tskit_. You don't have to worry about that here, but you can read [this](https://tskit.dev/tutorials/analysing_tree_sequences.html#sec-tutorial-afs-zeroth-entry) for more detail.

When used in R, your function should work like this:

```{r}
#| echo: false
simulate_afs <- function(Ne) {
  n <- 20 # 1 is for the fixed sites included by tskit
  theta <- 4 * 1e-8 * Ne * 100e6
  round(theta * 1/1:n)
}
```

```{r}
simulate_afs(Ne = 1000)
```



::: callout-note
#### Click to see the solution
```{r}
# An R function can be understood as a block of a computer program which executes
# a block of code inside the {...} brackets given a certain value of a parameter
# (here 'Ne' just after the word 'function')
simulate_afs <- function(Ne) {
  # create a slendr model with a single population of size Ne = N
  pop <- population("pop", N = Ne, time = 1)
  model <- compile_model(pop, generation_time = 1, simulation_length = 100000)

  # simulate a tree sequence
  ts <-
    msprime(model, sequence_length = 10e6, recombination_rate = 1e-8) %>%
    ts_mutate(mutation_rate = 1e-8)

  # get a random sample of names of 10 individuals
  samples <- ts_names(ts) %>% sample(10)

  # compute the AFS vector (dropping the 0-th element added by tskit)
  afs <- ts_afs(ts, sample_sets = list(samples))[-1]

  afs
}

# Let's use our custom function to simulate AFS vector for Ne = 1k, 10k, and 30k
afs_1k <- simulate_afs(1000)
afs_10k <- simulate_afs(10000)
afs_30k <- simulate_afs(30000)

# Plot the three simulated AFS using base R plotting functionality
plot(afs_30k, type = "o", main = "AFS, Ne = 30000", col = "cyan",)
lines(afs_10k, type = "o", main = "AFS, Ne = 10000", col = "purple")
lines(afs_1k, type = "o", main = "AFS, Ne = 1000", col = "blue")
legend("topright", legend = c("Ne = 1k", "Ne = 10k", "Ne = 30k"),
       fill = c("blue", "purple", "cyan"))
```
:::




## Part 2: Estimating unknown $N_e$ from empirical AFS

```{r}
#| eval: false
#| echo: false
set.seed(42)
TRUE_NE <- 6543

pop <- population("pop", N = TRUE_NE, time = 100000)
model <- compile_model(pop, generation_time = 1, direction = "backward")

ts <-
  msprime(model, sequence_length = 10e6, recombination_rate = 1e-8, random_seed = 42) %>%
  ts_mutate(mutation_rate = 1e-8, random_seed = 42)

samples <- ts_names(ts) %>% sample(10)

afs_observed <- ts_afs(ts, list(samples))
```

Imagine you sequenced 10 samples from a population and computed the following
AFS vector (which contains, sequentially, the number of singletons, doubletons,
etc., in your sample from a population):

<!-- dput(as.vector(observed_afs)) -->

```{r}
afs_observed <- c(2520, 1449, 855, 622, 530, 446, 365, 334, 349, 244,
                  264, 218,  133, 173, 159, 142, 167, 129, 125, 143)
```

You know (maybe from some fossil evidence) that the population probably had
a constant $N_e$ somewhere between 1000 and 30000 for the past 100,000 generations,
and had mutation and recombination rates of 1e-8 (i.e., parameters already
implemented by your `simulate_afs()` function).

**Use _slendr_ simulations to guess the true (and hidden!) $N_e$ given the observed
AFS by running simulations for a range of $N_e$ values and comparing each run to
the `afs_observed` vector above.**

**Hint:** Depending on your level of comfort with R (and programming in general), you can
choose one of the following approaches:

- **Option 1** [easy]: Plot AFS vectors for various $N_e$ values, then eyeball
which looks closest to the observed AFS based on the figures alone.

- **Option 2** [hard]: Simulate AFS vectors in steps of possible `Ne` (maybe
`lapply()`?), and find the $N_e$ which gives the closest AFS to the observed AFS based on [Mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error).



::: callout-note
#### Click to see the solution to "Option 1"
```{r}
# This is our starting observed AFS which we want to compare simulated AFS vectors to
afs_observed <- c(2520, 1449, 855, 622, 530, 446, 365, 334, 349, 244,
                  264, 218,  133, 173, 159, 142, 167, 129, 125, 143)

# We know that the Ne is between 1000 and 30000, so let's simulate
# a bunch of AFS vectors for different Ne values
afs_Ne1k <- simulate_afs(Ne = 1000)
afs_Ne5k <- simulate_afs(Ne = 5000)
afs_Ne6k <- simulate_afs(Ne = 6000)
afs_Ne10k <- simulate_afs(Ne = 10000)
afs_Ne20k <- simulate_afs(Ne = 20000)
afs_Ne30k <- simulate_afs(Ne = 30000)

# Plot all simulated AFS vectors, highlighting the observed AFS in black
plot(afs_observed, type = "b", col = "black", lwd = 3,
     xlab = "allele count bin", ylab = "count", ylim = c(0, 13000))
lines(afs_Ne1k, lwd = 2, col = "blue")
lines(afs_Ne5k, lwd = 2, col = "green")
lines(afs_Ne6k, lwd = 2, col = "pink")
lines(afs_Ne10k, lwd = 2, col = "purple")
lines(afs_Ne20k, lwd = 2, col = "orange")
lines(afs_Ne30k, lwd = 2, col = "cyan")
legend("topright",
       legend = c("observed AFS", "Ne = 1000", "Ne = 5000",
                  "Ne = 6000", "Ne = 10000", "Ne = 20000", "Ne = 30000"),
       fill = c("black", "blue", "green", "pink", "purple", "orange", "cyan"))


# !!!!! SPOILER ALERT BEFORE REVEALING THE CORRECT ANSWER !!!!!
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
# true Ne was 6543!
```
:::



::: callout-note
#### Click to see the solution to "Option 2"

```{r, eval=RERUN}
# This is our starting observed AFS which we want to compare simulated AFS vectors to
afs_observed <- c(2520, 1449, 855, 622, 530, 446, 365, 334, 349, 244,
                  264, 218,  133, 173, 159, 142, 167, 129, 125, 143)

# Generate regularly spaced values of potential Ne values (our parameter grid)
Ne_grid <- seq(from = 1000, to = 30000, by = 500)
Ne_grid

# I'm not entirely sure if your workshop cloud instances support big
# parallelization runs -- if not, you can modify the `mc.cores =` argument
# a couple of lines below to a smaller number (`mc.cores = 1` would make
# the simulation run on a single processor, i.e. no parallelization).
library(parallel)

# Compute AFS (in parallel, to make things faster) across the entire grid of possible Ne values
afs_grid <- mclapply(Ne_grid, simulate_afs, mc.cores = detectCores())
names(afs_grid) <- Ne_grid

# Show the first five simulated AFS vectors, for brevity
afs_grid[1:5]



# Plot the observed AFS...
plot(afs_observed, type = "b", col = "black", lwd = 3, xlab = "allele count bin", ylab = "count")
# ... and overlay the simulated AFS vectors on top of it
for (i in seq_along(Ne_grid)) {
  lines(afs_grid[[i]], lwd = 0.5)
}
legend("topright", legend = c("observed AFS", "simulated AFS"), fill = c("black", "gray"))




# Compute mean-squared error of the AFS produced by each Ne value across the grid
errors <- sapply(afs_grid, function(sim_afs) {
  sum((sim_afs - afs_observed)^2) / length(sim_afs)
})

plot(Ne_grid, errors, ylab = "error")
abline(v = Ne_grid[which.min(errors)], col = "red")
legend("topright", legend = paste("minimum error Ne =", Ne_grid[which.min(errors)]), fill = "red")




# Plot the AFS again, but this time highlight the most likely spectrum
# (i.e. the one which gave the lowest RMSE value)
plot(afs_observed, type = "b", col = "black", lwd = 3, xlab = "allele count bin", ylab = "count")
for (i in seq_along(Ne_grid)) {
  color <- if (i == which.min(errors)) "red" else "gray"
  width <- if (i == which.min(errors)) 2 else 0.75
  lines(afs_grid[[i]], lwd = width, col = color)
}
legend("topright", legend = c("observed AFS", paste("best fitting Ne =", Ne_grid[which.min(errors)])),
       fill = c("black", "red"))


# !!!!! SPOILER ALERT BEFORE REVEALING THE CORRECT ANSWER !!!!!
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
# true Ne was 6543!
```
:::

































# Exercise 4: Simulating dynamics of positive selection

The primary motivation for designing _slendr_ was to make demographic modelling
in R as trivially easy and fast as possible, focusing exclusively on neutral
models. However, as _slendr_ became popular, people have been asking for
the possibility of simulating natural selection. After all, a large
part of _slendr_'s functionality deals with population genetic [models across
geographical landscapes](https://www.slendr.net/articles/vignette-06-locations.html),
which requires SLiM. Even though these models are
neutral, SLiM is _the_ most famous and popular tool for simulating various
aspects of natural selection.

Recently I caved in and added support for modifying
_slendr_ demographic models with bits of SLiM code which allows simulating
pretty much any arbitrary selection scenario you might be interested in. 
This exercise is a quick demonstration of how this work and how you might
simulate selection using _slendr_. We will do this using another toy
model of ancient human history, which we will use as a basis for simulating
the frequency trajectory of an allele under positive selection, and implementing
a simpleselection scan summary statistic using Tajima's D.

To speed things up, **create a new `exercise4.R` script and copy the following
code as a starting point for this exercise**:

```{r}
#| collapse: true
library(slendr)
init_env(quiet = TRUE)

# This line sources a script in which I provide a few useful helper functions
# which you can use in this exercise
source(here::here("utils.R"))

# African ancestral population
afr <- population("AFR", time = 65000, N = 5000)

# First migrants out of Africa
ooa <- population("OOA", parent = afr, time = 60000, N = 5000, remove = 27000)

# Eastern hunter-gatherers
ehg <- population("EHG", parent = ooa, time = 28000, N = 5000, remove = 6000)

# European population
eur <- population("EUR", parent = ehg, time = 25000, N = 5000)

# Anatolian farmers
ana <- population("ANA", time = 28000, N = 5000, parent = ooa, remove = 4000)

# Yamnaya steppe population
yam <- population("YAM", time = 8000, N = 5000, parent = ehg, remove = 2500)

# Define gene-flow events
gf <- list(
  gene_flow(from = ana, to = yam, rate = 0.75, start = 7500, end = 6000),
  gene_flow(from = ana, to = eur, rate = 0.5, start = 6000, end = 5000),
  gene_flow(from = yam, to = eur, rate = 0.6, start = 4000, end = 3500)
)

# Compile all populations into a single slendr model object
model <- compile_model(
  populations = list(afr, ooa, ehg, eur, ana, yam),
  gene_flow = gf, generation_time = 30
)

# Schedule the sampling from four European populations roughly before their
# disappearance (or before the end of the simulation)
schedule <- rbind(
  schedule_sampling(model, times = 0, list(eur, 50)),
  schedule_sampling(model, times = 6000, list(ehg, 50)),
  schedule_sampling(model, times = 4000, list(ana, 50)),
  schedule_sampling(model, times = 2500, list(yam, 50))
)
```

**Next, visualize the demographic model.** If you did a bit of work in human
population genetics, you might recognize it as a very simplified model
of demographic history of Europe over the past 50 thousand years or so.
As you can see, we are recording 50 individuals from four populations -- for
Europeans we sample 50 individuals at "present-day", for the remaining populations
we're recording 50 individuals just before their disappearance. Also note that
there's quite a bit of gene-flow! This was an important thing we've learned about
human history in the past 10 years or so -- everyone is mixed with pretty much
everyone, there isn't (and never was) anything as a "pure population".

```{r}
#| fig-width: 6
#| fig-height: 6
plot_model(model, proportions = TRUE, samples = schedule)
```




### Part 1: Simulating a tree sequence and computing Tajima's D 

Although the point of this exercise is to simulate selection, let's first
simulate a normal neutral model using slendr's `msprime()` engine as a sanity
check. **Simulate 10 Mb of sequence with a recombination rate `1e-8` and a
sampling `schedule` defined above.** Let's not worry about adding any mutations, just to change things up a little bit. We'll be working with
branch-based statistics here (which means adding `mode = "branch"` whenever
we will be computing a statistic, such as Tajima's D).

::: callout-note
#### Click to see the solution

```{r}
ts <- msprime(model, sequence_length = 10e6, recombination_rate = 1e-8, samples = schedule)
```
:::

**Inspect the table of all individuals recorded in our tree sequence using
the function `ts_samples()`, making use we have all the individuals scheduled
for tree-sequence recording.**

::: callout-note
#### Click to see the solution

```{r}
ts_samples(ts)

library(dplyr)
ts_samples(ts) %>% group_by(pop, time) %>% tally
```
:::


As you've learned, _tskit_ functions in _slendr_ generally operate on vectors
(or lists) of individual names, like those produced by `ts_names()` above.
**Get a vector of names of individuals in every population recorded in the
tree sequence, then use this to compute Tajima's D using the _slendr_ function
`ts_tajima()`. Do you see any striking differences in the Tajima's D values across populations? Check [this](https://en.wikipedia.org/wiki/Tajima%27s_D#Interpreting_Tajima's_D)
for some general guidance.**

::: callout-note
#### Click to see the solution

```{r}
samples <- ts_names(ts, split = "pop")
samples

# Compute genome-wide Tajima's D for each population -- note that we don't
# expect to see any significant differences because no population experienced
# natural selection (yet)
ts_tajima(ts, sample_sets = samples, mode = "branch")
```
:::




## Part 2: Computing Tajima's D in windows

Let's take this one step forward. Even if there is a locus under positive selection
somewhere along our chromosome, it might be quite unlikely that we would find a
Tajima's D value significant enough for the entire chromosome (which is basically
what we did in the previous Part 1). Fortunately, thanks to the flexibility of
the _tskit_ module, the _slendr_ function  `ts_tajima()` has an argument
`windows =`, which allows us to specify the coordinates of windows into which
a sequence should be broken into, with Tajima's D computed separately for each
window. Perhaps this will allow us to see the impact of positive selection?

**Define a variable `windows` which will contain a vector of coordinates of
100 windows, starting at position `0`, and ending at position `10e6` (i.e., the end
of our chromosome). Then provide this variable as the `windows =` argument of
`ts_tajima()` on a new, separate line of your script. Save the result of
`ts_tajima()` into the variable `tajima_wins`, and inspect its contents in the
R console.***

**Hint:** You can use the R function `seq()` and its argument `length.out = 100`,
to create the coordinates of window boundaries very easily.

::: callout-note
#### Click to see the solution

```{r}
# Pre-compute genomic windows for window-based computation of Tajima's D
windows <- round(seq(0, ts$sequence_length, length.out = 100))
windows

# Compute genome-wide Tajima's D for each population in individual windows
tajima_wins <- ts_tajima(ts, sample_sets = samples, windows = windows, mode = "branch")
tajima_wins

# You can see that the format of the result is slightly strange, with the
# `D` column containing a vector of numbers (this is done for conciseness)
tajima_wins[1, ]$D
```
:::

The default output format of `ts_tajima()` is not super user-friendly. **Process
the result using a helper function `process_tajima(tajima_wins)` that I provided
for you (perhaps save it as `tajima_df`), and visualize it using another
of my helper functions `plot_tajima(tajima_df)`.**

::: aside
**Note:** Making the `process_tajima()` and `plot_tajima()` function available
in your R code is the purpose of the `source(here::here("utils.R"))` command
at the beginning of your script for this exercise.
:::

::: callout-note
#### Click to see the solution

```{r}
# The helper function `process_tajima()` reformats the results into a normal
# data frame, this time with a new column `window` which indicates the index
# of the window that each `D` value was computed in
tajima_df <- process_tajima(tajima_wins)
tajima_df

# Now let's visualize the window-based Tajima's D along the simulated genome
# using another helper function `plot_tajima()` (hint: we still don't expect
# anything interesting here because we ran a purely neutral simulation, so this
# is more like a control)
plot_tajima(tajima_df)
```
:::

















## Part 3: Adding positive selection to the base demographic model

Although primarily designed for neutral demographic models, _slendr_ allows
optional simulation of natural selection by providing a "SLiM extension code
snippet" with customization SLiM code as an optional argument `extension =`
of `compile_model()` (a function you're closely familiar with at this point).

Unfortunately we don't have any space to explain SLiM here (and I have no idea,
at the time of writing, whether or not you will have worked with SLiM earlier
in this workshop). Suffice to say that SLiM is another very popular population
genetic simulator software which allows simulation of selection, and which
requires you to write custom code in a different programming language called
Eidos.


**Take a look at the file `slim_extension.txt` provided in your working
directory (it's also part of the GitHub repository [here](https://github.com/bodkan/cesky-krumlov-2025/blob/main/slim_extension.txt)).
If you worked with SLiM before, glance through the script casually and see
if it makes any sense to you. If you have not worked with SLiM before,
look for the strange `{{elements}}` in curly brackets in the first ten lines
of the script.** Those are the parameters of the selection model we will be
customizing are standard neutral demographic model from in the next step.

Specifically, when you inspect the `slim_extension.txt` file, you can see
that this "SLiM extension script" I provided for you has three parameters:

- `origin_pop` -- in which population should a beneficial allele appear,
- `s` -- what should be the selection coefficient of the beneficial allele, and
- `onset_time` -- at which time should the allele appear in the `origin_pop`.

However, at the start, the SLiM extension snippet doesn't contain any concrete
values of those parameters, but only their `\{{origin_pop}\}`, `\{{s}\}`, and
`\{{onset_time}\}` placeholders.

**Use the _slendr_ function `substitute_values()` to substitute concrete values
for those parameters like this:**

```{r}
extension <- substitute_values(
  template = here::here("slim_extension.txt"),
  origin_pop = "EUR",
  s = 0.15,
  onset_time = 12000
)
extension
```

You can see that `substitute_values()` returned a path to a file. **Take a look
at that file in your terminal -- you should see each of the three `{{placeholder}}`
parameters replaced with a concrete given value.**

::: callout-note
#### Click to see the solution

Let's take a look at the first 15 lines of the extension file before and
after calling `substitute_values()`. We'll do this in R for simplicity, but
you can use `less` in plain unix terminal.

```{R}
# Before -- see the {{placeholder}} parameters in their original form
cat(paste(readLines("slim_extension.txt")[1:15], collapse = "\n"))

# After -- see the {{placeholder}} parameters with concrete values!
cat(paste(readLines(extension)[1:15], collapse = "\n"))
```
:::

And that's all the extra work we need to turn our purely neutral demographic
_slendr_ model into a model which includes natural selection! (In this case,
only a simple selection acting on a single locus, as you'll see later, but
this can be generalized to any imaginable selection scenario.)

How do we use the SLiM extension for our simulation? It's very simple -- we
just have to provide the `extension` variable as an additional argument of
good old `compile_model()`. This will compile a new _slendr_ model which will
now include the new functionality for simulating natural selection:

**Compile a new `model` of the history of populations `afr`, `ooa`, `ehg`, 
etc., by following the instructions above, providing a new `extension =`
argument to the `compile_model()` function.**

::: callout-note
#### Click to see the solution

```{r}
model <- compile_model(
  populations = list(afr, ooa, ehg, eur, ana, yam),
  gene_flow = gf, generation_time = 30,
  extension = extension   # <======== this is missing in the neutral example!
)
```
:::



## Part 4: Running a selection simulation using `slim()`

Now we can finally run our selection simulation!

There are two modifications to our previous simulation workflows:

1. Because we need to run a non-neutral simulation, we have to switch from using
the `msprime()` _slendr_ engine to `slim()`. The latter can still interpret the
same demographic model we programmed in R, just like the `msprime()` engine can,
but will run the model using SLiM (and thus leveraging the new SLiM extension code
that we have customized using `substitute_values()` above). We simply do this by
switching from this:

```{r}
#| eval: false
ts <- msprime(model, sequence_length = 10e6, recombination_rate = 1e-8, samples = schedule)
```

to this:

```{r}
#| eval: false
ts <- slim(model, sequence_length = 10e6, recombination_rate = 1e-8, samples = schedule)
```

As you can see, you don't have to modify anything in your model code, just
switching from `msprime` to `slim` in the line of code which produces the
simulation result.

2. The customized model will not only produce a tree sequence, but will
also generate a table of allele frequencies in each population (SLiM experts
might have noticed the revelant SLiM code when they were inspecting
[`slim_extension.txt`](https://github.com/bodkan/cesky-krumlov-2025/blob/main/slim_extension.txt)). We need to be able to load both of these files after
the simulation and thus need a path to a location we can find those files.
We can do this by calling the `slim()` function as `path <- slim(..., path = TRUE)`
(so with the extra `path =` argument). This will return a path to where the
`slim()` engine saved all files with our desired results.

**Run a simulation from the modified model of selection with the `slim()` engine
as instructed in points number 1. and 2. above, then use the `list.files(path)`
function in R to take a look in the directory. Which files were produced by
the simulation?**

::: callout-note
#### Click to see the solution (you have a working SLiM installation)

```{r, eval=RERUN}
# tstart <- Sys.time()
path <- slim(model, sequence_length = 10e6, recombination_rate = 1e-8, samples = schedule, path = TRUE, random_seed = 59879916)
# tend <- Sys.time()
# tend - tstart # Time difference of 38.82014 secs

# We can verify that the path not only contains a tree-sequence file but also
# the table of allele frequencies.
list.files(path)
```
:::

::: callout-note
#### Click to see the solution (you don't have a working SLiM installation)

```{r}
# If you don't have SLiM set up, just use the simulated results from my own
# run of the same simulation
path <- here::here("data/selection")

# We can verify that the path not only contains a tree-sequence file but also
# the table of allele frequencies.
list.files(path)
```
:::

## Part 5: Investigating allele frequency trajectories

**Use another helper function `read_trajectory(path)` which I provided for this
exercise to read the simulated frequency trajectories of the positively
selected mutation in all of our populations into a variable `traj_df`. Then
run a second helper function `plot_trajectory(traj_df)` to inspect the trajectories
visually.**

**Recall that you used the function `substitute_values()` to parametrize your
selection model so that the allele under selection occurs in Europeans 15 thousand
years ago, and is programmed to be under very strong selection of $s = 0.15$.
Do the trajectories visualized by `plot_trajectory()` make sense given the
demographic model of European prehistory plotted above?**

::: callout-note
#### Click to see the solution

```{r}
traj_df <- read_trajectory(path)
traj_df

plot_trajectory(traj_df)

# Comparing the trajectories side-by-side with the demographic model reveals
# some obvious patterns of both selection and demographic history.
plot_grid(
  plot_model(model),
  plot_trajectory(traj_df),
  nrow = 1, rel_widths = c(0.7, 1)
)
```
:::

## Part 6: Tajima's D (genome-wide and window-based) from the selection model

Recall that your simulation run saved results in the location stored in the
`path` variable:

```{r}
list.files(path)
```

From this `path`, we've already successfuly investigated the frequency trajectories.

Now let's compute Tajima's D on the tree sequence simulated from our selection
model. Hopefully we should see an interesting pattern in our selection scan?
For instance, we don't know yet _where_ in the genome is the putative locus
under selection!

To read a tree sequence simulated with `slim()` by our customized selection setup,
we need to do a bit of work. To simplify things a bit, here's the R code which makes
it possible to do. Just copy it in your `exercise4.R` script as it is:

```{r}
# Let's use my own saved simulation results, so that we're all on the
# same page going forward
path <- here::here("data/selection")

ts <-
  file.path(path, "slim.trees") %>%  # 1. compose full path to the slim.trees file
  ts_read(model) %>%                 # 2. read the tree sequence file into R
  ts_recapitate(Ne = 5000, recombination_rate = 1e-8) # 3. perform recapitation
```

Very briefly, because our tree sequence was generated by SLiM, it's very likely
that not all genealogies along the simulated genome will be fully coalesced
(i.e., not all tree will have a single root). To explain why this is the case
is out of the scope of this session, but read [here](https://tskit.dev/pyslim/docs/latest/tutorial.html) if you're interested
in learning more. For the time being, it suffices to say that we can pass the
(uncoalesced) tree sequence into the `ts_recapitate()` function, which then
takes a SLiM tree sequence and simulates all necessary "ancestral history" that
was missing on the uncoalesced trees, thus ensuring that the entire tree
sequence is fully coalesced and can be correctly computed on.

**Now that you have a `ts` tree sequence object resulting from a new selection
simulation run, repeat the analyses of genome-wide and window-based Tajima's D
from _Part 1_ and _Part 2_ of this exercise, again using the provided helper
functions `process_tajima()` and `plot_tajima()`. Can you identify which locus
has been the likely focal point of the positive selection? Which population
shows evidence of selection? Which doesn't and why (look again at the 
visualization of the demographic model above)?**

::: callout-note
#### Click to see the solution

```{r}
samples <- ts_names(ts, split = "pop")
samples

# Overall Tajima's D across the 10Mb sequence still doesn't reveal any significant
# deviations even in case of selection (again, not entirely unsurprising)
ts_tajima(ts, sample_sets = samples, mode = "branch")
```


```{r}
# So let's look at the window-based computation again...
windows <- as.integer(seq(0, ts$sequence_length, length.out = 100))

# compute genome-wide Tajima's D for each population in individual windows
tajima_wins <- ts_tajima(ts, sample_sets = samples, windows = windows, mode = "branch")
tajima_df <- process_tajima(tajima_wins)

plot_tajima(tajima_df)
```

You should see a clear dip in Tajima's D around the midpoint of the DNA sequence,
but only in Europeans. The beneficial allele appeared in the European population,
and although the plot of the allele frequency trajectories shows that the selection
dynamics has been _dramatically_ affected by gene-flow events (generally causing
a repeated "dilution" of the selection signal in Europeans), there has never been
gene-flow (at least in our model) _from_ Europeans to other populations, so the
beneficial allele never had a chance to "make it" into those populations.

:::










:::::: callout-tip
## Bonus exercises

#### Bonus 1: Investigate the impact of recombination around the selected locus

Vary the uniform recombination rate and observe what happens with Tajima's D
in windows along the genome.

::: callout-note
#### Click to see the solution

Solution: just modify the value of the `recombination_rate =` argument provided
to the `slim()` function above.
:::





#### Bonus 2: Simulate origin of the allele in EHG


Simulate the origin of the beneficial allele in the EHG population -- what
do the trajectories look like now? How does that change the Tajima's D
distribution along the genome in our European populations?

::: callout-note
#### Click to see the solution

Use this extension in the `slim()` call, and repeat the rest of the
selection-based workflow in this exercise.

```{r}
#| eval: false
extension <- substitute_values(
  template = "slim_extension.txt",
  origin_pop = "EHG",
  s = 0.1,
  onset_time = 12000
)
model <- compile_model(
  populations = list(afr, ooa, ehg, eur, ana, yam),
  gene_flow = gf, generation_time = 30,
  extension = extension
)
```
:::



#### Bonus 3: Other statistics in windows

As a practice of your newly acquired tree-sequence computation skills with
_slendr_, calculate some other statistics in the same windows along the
simulated genome, visualize them yourself, and compare the results to the
window-based Tajima's D pattern. For instance, `ts_diversity()`, `ts_divergence()`,
or `ts_segregating()` might be quite interesting to look at.

::: callout-note
#### Click to see the solution

Use the same tree sequence file you've computed Tajima's D on, and then
apply the functions `ts_diversity()`, `ts_divergence()`, and `ts_segregating()`
on that tree sequence.
:::




:::
<!-- End of Bonus exercises for Exercise 4 -->























# Exercise 5: Programming of your own model

Do you have a favourite model organism or you study another species? **Take some of the most important features of its demographic history and program them as a new _slendr_ model.**

**Then, think about some interesting population genetic statistics that you
either know from the literature or that you perhaps computed on your own
empirical data. Try to compute them on a tree sequence simulated from your 
model using `msprime()`. Do you get results which are in the ballpark of
your expectations?**

::: callout-note
### Click to see the solution

I don't have a solution, of course! :) But I would be really happy if you
send me a _slendr_ script with your model, especially if you get to use it
in your own projects!

If you would be OK with it, I think it would be great to create a gallery
of models created by people for all kinds of different species and publish  
it on the [_slendr_ website](https://slendr.net).
:::




















